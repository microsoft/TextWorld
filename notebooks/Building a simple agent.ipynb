{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple agent with TextWorld\n",
    "This tutorial outlines how to build an agent that learns how to play __choice-based__ text-based games generated with TextWorld."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning challenges\n",
    "Training an agent such that it can learn how to play text-based games is not trivial. Among other challenges, we have to deal with\n",
    "\n",
    "1. a combinatorial action space (that grows w.r.t. vocabulary)\n",
    "2. a really sparse reward signal.\n",
    "\n",
    "To ease the learning process, TextWorld offers control over what information is availble to the agent during training.    \n",
    "\n",
    "#### Admissible commands\n",
    "We can provide the agent with the list of __valid__ actions that can be performed at every game state. This is done by calling `env.activate_state_tracking()` before `env.reset()`.\n",
    "\n",
    "_*Only available for games generated with TextWorld._ \n",
    "\n",
    "#### Intermediate reward\n",
    "We can compute an intermediate reward for the agent. This is done by calling `env.compute_intermediate_reward()` before `env.reset()`. The intermediate reward can either be:\n",
    "- __-1__: last action needs to be undone before resuming the quest\n",
    "-  __0__: last action didn't affect the quest\n",
    "-  __1__: last action brought us closer to the goal\n",
    "\n",
    "_*Only available for games generated with TextWorld._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test games\n",
    "We handcrafted 6 incremental games using the following world:\n",
    "```\n",
    "                     Bathroom\n",
    "                        +\n",
    "                        |\n",
    "                        +\n",
    "    Bedroom +-(d1)-+ Kitchen +--(d2)--+ Backyard\n",
    "      (P)               +                  +\n",
    "                        |                  |\n",
    "                        +                  +\n",
    "                   Living Room           Garden\n",
    "```\n",
    "* `games/baby.ulx`: __Escape__ the bedroom (5 actions).\n",
    "* `games/short.ulx`: __Escape__ + __open d2__ (6 actions). \n",
    "* `games/medium.ulx`: __Escape__ + __open d2__ + __take carrot__ found in the garden (9 actions).\n",
    "* `games/long.ulx`: __Escape__ + __open d2__ + __take carrot__ + __put carrot__ on the stove in the kitchen (12 actions).\n",
    "* `games/last.ulx`: Same as the __long__ version but only the last step is described in the objective.\n",
    "* `games/human.ulx`: Same as the __long__ but the objective is not provided. One must search for a clue to figure out what to do. Hint: one should look for a note!\n",
    "\n",
    "_* NB: Agent can lose the game if it eats the carrot instead of putting it on the stove._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the random baseline\n",
    "Let's start with building an agent that simply selects a valid command at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import textworld\n",
    "\n",
    "\n",
    "class RandomAgent(textworld.Agent):\n",
    "    \"\"\" Agent that randomly selects commands from the admissible ones. \"\"\"\n",
    "    def __init__(self, seed=1234):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(self.seed)\n",
    "\n",
    "    def reset(self, env):\n",
    "        # Activate state tracking in order to get the admissible commands.\n",
    "        env.activate_state_tracking()\n",
    "        env.compute_intermediate_reward()  # Needed to detect if a game is lost.\n",
    "\n",
    "    def act(self, game_state, reward, done):\n",
    "        return self.rng.choice(game_state.admissible_commands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function\n",
    "Let's write simple function to test our agent on a test game (i.e. `./games/long.ulx`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, game, max_step=500, nb_episodes=10):\n",
    "    env = textworld.start(game)  # Start the game.\n",
    "    print(game.split(\"/\")[-1], end=\"\")\n",
    "    \n",
    "    # Collect some statistics: nb_steps, final reward.\n",
    "    avg_moves, avg_scores = [], []\n",
    "    for no_episode in range(nb_episodes):\n",
    "        agent.reset(env)          # Tell the agent a new episode is starting.\n",
    "        game_state = env.reset()  # Start new episode.\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "        for no_step in range(max_step):\n",
    "            command = agent.act(game_state, reward, done)\n",
    "            game_state, reward, done = env.step(command)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # print(\"Done after {} steps. Score {}/1.\".format(game_state.nb_moves, game_state.score))\n",
    "        print(\".\", end=\"\")\n",
    "        avg_moves.append(game_state.nb_moves)\n",
    "        avg_scores.append(game_state.score)\n",
    "\n",
    "    env.close()\n",
    "    print(\"  \\tavg. steps: {:5.1f}; avg. score: {:4.1f} / 1.\".format(np.mean(avg_moves), np.mean(avg_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent(RandomAgent(), game=\"./games/baby.ulx\")\n",
    "test_agent(RandomAgent(), game=\"./games/short.ulx\")\n",
    "test_agent(RandomAgent(), game=\"./games/medium.ulx\")\n",
    "test_agent(RandomAgent(), game=\"./games/long.ulx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the oracle agent on the test games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent(textworld.agents.WalkthroughAgent(), game=\"./games/baby.ulx\", nb_episodes=1)\n",
    "test_agent(textworld.agents.WalkthroughAgent(), game=\"./games/short.ulx\", nb_episodes=1)\n",
    "test_agent(textworld.agents.WalkthroughAgent(), game=\"./games/medium.ulx\", nb_episodes=1)\n",
    "test_agent(textworld.agents.WalkthroughAgent(), game=\"./games/long.ulx\", nb_episodes=1)\n",
    "test_agent(textworld.agents.WalkthroughAgent(), game=\"./games/last.ulx\", nb_episodes=1)\n",
    "test_agent(textworld.agents.WalkthroughAgent(), game=\"./games/human.ulx\", nb_episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural agent\n",
    "TextWorld allows anyone to generate text-based games with varying complexity. This allows us to train a neural reinforcement learning agent that doesn't just memorize the steps to solving a game, but generalizes to different games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum learning\n",
    "It would make more sense to __avoid training on the test games__ and rather train the agent on a different set of games, then report the agent's performance on the test games.\n",
    "\n",
    "One can generate several sets of games having different level of difficulty. Then, during training, we start by playing the easier games and gradually move toward harder games.\n",
    "\n",
    "#### Generate games for training\n",
    "To generate random games for training the agent, we can use the script `tw_make` (shipped with TextWorld). The main arguments for the script are:\n",
    "* `--world-size`: number of rooms in the game.\n",
    "* `--nb-objects`: number of objects in the game.\n",
    "* `--quest-length`: number of steps needed to solve the game.\n",
    "* `--seed`: seed controlling the game generation process.\n",
    "* `--theme`: specify which text grammar to use (default `house`).\n",
    "* `--output`: folder where to save generated games.\n",
    "\n",
    "##### Example - Generating 100 training games in parallel using bash:\n",
    "```bash\n",
    "seq 1 100 | xargs -n1 -P4 tw-make custom --world-size 2 --nb-objects 10 --quest-length 3 --output ./obj_10_qlen_3_room_2/ --seed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your own agent\n",
    "\n",
    "The TextWorld environment provides a number of properties for your agent to work on.\n",
    "\n",
    "### `GameState` properties\n",
    "* `description`: Text description of the current room.\n",
    "* `inventory`: Text description of the inventory.\n",
    "* `feedback`: Text feedback of the parser in response to the last text command.\n",
    "* `objective`: Text instructions to follow in order to win the game.\n",
    "* `admissible_commands`: A list of candidate text commands.\n",
    "* `reward`: 1.0 if the agent wins the game at current step; -1.0 if it loses the game; 0.0 otherwise.\n",
    "* `intermediate_reward`: {-1.0, 0.0, 1.0}, the oracle reward as described [above](#Oracle-reward).\n",
    "* `done`: True, if the agent either won or lost the game; False, otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textworld\n",
    "\n",
    "\n",
    "class NeuralAgent(textworld.Agent):\n",
    "    def __init__(self):\n",
    "        # Initialize your agent.\n",
    "        pass\n",
    "\n",
    "    def reset(self, env):\n",
    "        # For the purpose of this tutorial we need the two lines below.\n",
    "        env.activate_state_tracking()  # Needed to get the admissible commands.\n",
    "        env.compute_intermediate_reward()  # Needed to detect if a game is lost and to get intermediate reward.\n",
    "\n",
    "    def act(self, game_state, reward, done):\n",
    "        # Given the current game_state return the next text command.\n",
    "        # Perfom inference. *Do not update model's parameters.*\n",
    "        return game_state.admissible_commands[0]\n",
    "    \n",
    "    def finish(self, game_state, reward, done):\n",
    "        # The game has finished. If done is True, agent won/lost the game; otherwise step limit was reached.\n",
    "        # This is where you should *update your model's parameters*.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import textworld\n",
    "\n",
    "\n",
    "def train_agent(agent, filenames_pattern, max_step=400, max_epoch=10):\n",
    "    rng = np.random.RandomState(1234)\n",
    "    try:\n",
    "        msg = \"# At any point you can hit Ctrl + C (stop the kernel) to break out of training early.\"\n",
    "        print(msg)\n",
    "        print(\"Training...\")\n",
    "        train_games = glob.glob(filenames_pattern)\n",
    "    \n",
    "        for epoch in range(max_epoch):\n",
    "            rng.shuffle(train_games)\n",
    "            for game in train_games:\n",
    "                game_name = os.path.basename(game)\n",
    "                env = textworld.start(game)\n",
    "                agent.reset(env)  # tells the agent a new run is starting.\n",
    "                game_state = env.reset()  # Start new run.\n",
    "\n",
    "                total_reward = 0\n",
    "                reward = 0\n",
    "                done = False\n",
    "                for t in range(max_step):\n",
    "                    command = agent.act(game_state, reward, done)\n",
    "                    game_state, reward, done = env.step(command)\n",
    "                    total_reward += reward\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                # Tell the agent the run is done.\n",
    "                agent.finish(game_state, reward, done)\n",
    "\n",
    "                msg = \"#{:2d}. {}:\\t {:3d} steps; score: {:2d}\"\n",
    "                msg = msg.format(epoch, game_name, game_state.nb_moves, total_reward)\n",
    "                print(msg)\n",
    "                env.close()\n",
    "\n",
    "    # At any point you can hit Ctrl + C to break out of training early.\n",
    "    except KeyboardInterrupt:\n",
    "        print('--------------------------------------------\\n')\n",
    "        print('Exiting from training early\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = NeuralAgent()  # Instantiate the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the agent on generated games for how long as you want (or use early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! seq 1 100 | xargs -n1 -P4 tw-make custom --world-size 2 --nb-objects 10 --quest-length 3 --output ./obj_10_qlen_3_room_2/ --seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent(agent, \"obj_10_qlen_3_room_2/*ulx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent(agent, game=\"./games/techfest2018_baby.ulx\")\n",
    "#test_agent(agent, game=\"./games/techfest2018_short.ulx\")\n",
    "#test_agent(agent, game=\"./games/techfest2018_medium.ulx\")\n",
    "#test_agent(agent, game=\"./games/techfest2018_long.ulx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Papers about RL applied to text-based games\n",
    "* [Language Understanding for Text-based games using Deep Reinforcement Learning][narasimhan_et_al_2015]\n",
    "* [Learning How Not to Act in Text-based Games][haroush_et_al_2017]\n",
    "* [Deep Reinforcement Learning with a Natural Language Action Space][he_et_al_2015]\n",
    "* [What can you do with a rock? Affordance extraction via word embeddings][fulda_et_al_2017]\n",
    "* [Text-based adventures of the Golovin AI Agent][kostka_et_al_2017]\n",
    "* [Using reinforcement learning to learn how to play text-based games][zelinka_2018]\n",
    "\n",
    "[narasimhan_et_al_2015]: https://arxiv.org/abs/1506.08941\n",
    "[haroush_et_al_2017]: https://openreview.net/pdf?id=B1-tVX1Pz\n",
    "[he_et_al_2015]: https://arxiv.org/abs/1511.04636\n",
    "[fulda_et_al_2017]: https://arxiv.org/abs/1703.03429\n",
    "[kostka_et_al_2017]: https://arxiv.org/abs/1705.05637\n",
    "[zelinka_2018]: https://arxiv.org/abs/1801.01999"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
